# EvaluationAI

- Evaluated GPT-4.1-nano and GPT-4o-mini on the ARC-Challenge dataset (2,590 questions) using UK AISIâ€™s Inspect.
- Conducted comparative benchmarking to analyze performance differences and asymmetric capabilities.
- Performed paired error analysis on questions only one model solved; examined dataset quality in shared failures.
- Developed log-probability-based evaluation pipeline for estimating model confidence over answer choices.
- Investigated few-shot prompting effects on confidence calibration and correlation with accuracy.
